---
title: 'Tutorial 3: Text Analysis'
author: "Ryan Cordell, Jonathan Fitzgerald, Lindsay Thomas"
date: "3/4/2019"
output: html_document
---
# Note

I've included an RMD file with the answers to coding problems posed in this tutorial in the repo you downloaded from Github (in the `exercise-answers` folder). Feel free to use it if you get stuck, but make sure you can explain what happens in each line.

# Acknowledgements

This tutorial is adapted from Ryan Cordell and Jonathan Fitzgerald's "Exploratory.Rmd" exercise from Cordell's Spring 2017 course, ["Humanities Data Analysis"](http://s17hda.ryancordell.org/), on which it draws heavily. See the original exercise in that course's Github repo: ["Exploratory.Rmd"](https://github.com/rccordell/s17hda/blob/master/exercises/Exploratory.Rmd).

# Set Up

Load `tidyverse` before we get started, and make sure your working directory is set correctly (`getwd()` and `setwd()`).

```{r}
library(tidyverse)
```

# Confusion and Looking Things Up

This tutorial picks up the pace considerably. A lot is going to happen quickly, and it might be difficult to follow it all. That's ok. Make sure that you can follow the *logic* of each code block, if not the actual, literal code itself. If you get confused, remember to use R's `help()` function, and/or to use Google/Stack Overflow. I look things up **all of the time** when I'm doing this kind of work; it's basically the only way I learn how to do anything. Learning to program -- especially as an academic -- is at least 75% learning how to look things up on Stack Overflow and/or Google, weed through all of the unhelpful answers or those that are not actually about your specific problem, and find the helpful ones. 

# The Pipe Operator

Last week we explictly invoked each function in a new line of code. Today we'll introduce the pipe operator `%>%`, which allows us to chain together a series of transformations. Let's illustrate this with a few familiar blocks from last week (remember to change the filepath in the `read.csv` functions below to the correct filepath on your machine):

```{r}
census <- read.csv(file = "/Users/lxt308/Dropbox/Teaching/2019-spring/eng-682/tutorials/data/1840-census-data.csv")
census <- select(census, 1,6:81)
census <- dplyr::rename(census, county = QualifyingAreaName)
census_long <- gather(census, "identification", "count", 2:77)
census_long <- separate(census_long, county, into = c("county", "state"), sep = "\\, ")
census_long <- na.omit(census_long)
```

Using pipes, we can chain together these operations like so, to create two variables:

```{r}
census <- read.csv(file = "/Users/lxt308/Dropbox/Teaching/2019-spring/eng-682/tutorials/data/1840-census-data.csv") %>%
  select(1,6:81) %>%
  dplyr::rename(county = QualifyingAreaName)

census_long <- census %>%
  gather("identification", "count", 2:77) %>%
  separate(county, into = c("county", "state"), sep = "\\, ") %>%
  na.omit(census_long)
```

Or like so, to create only one `census_long` variable (note that you'll have to clear your `Global Environment` before running this code to see it actually work. Do that by clicking on the broom icon in the Environment pane.)

```{r}
census_long <- read.csv(file = "/Users/lxt308/Dropbox/Teaching/2019-spring/eng-682/tutorials/data/1840-census-data.csv") %>%
  select(1,6:81) %>%
  dplyr::rename(county = QualifyingAreaName) %>%
  gather("identification", "count", 2:77) %>%
  separate(county, into = c("county", "state"), sep = "\\, ") %>%
  na.omit(census_long)
```

Note that there are some distinct structural differences when using pipes. For one, the variable being transformed is usually invoked at the beginning of the chain, and thus does not appear as an argument in the separate parts of the chain. Compare these piped operations with the line-by-line operations above again. How do they compare? Where do you see `census` or `census_long` invoked in lines 33-37 when they are not in lines 56-61?  Write your answers below the heading.

### A:


We can also use pipes to make (and view) temporary transformations in our data that won't be saved as variables. This is a very useful way of seeing what a series of operations will do before "really" running them.

```{r}
census_long %>% 
  tidyr::spread(identification, count) %>%
  View()
```

Note that there is no variable assignment in the code block above, meaning the transformation is not being saved for later in a variable.

# Exploring Texts

Last week we worked with data frames that contained mostly numerical data and other bits of short textual information. Now we're going to start digging into longer texts, a common subject for humanistic data analysis. We'll begin working with a single text, and then learn how to read in a set of texts. The texts we'll be using are derived from the [Wright American Fiction project](http://webapp1.dlib.indiana.edu/TEIgeneral/projectinfo.do?brand=wright) out of Indiana University. This data is included in the `data` folder you downloaded with the tutorials from the course Github repo.

## Reading in a Single Text

To read in a text, we're going to use the `scan` function as follows (remember to change the filepath to the correct one for your machine):

```{r}

text <- scan("/Users/lxt308/Dropbox/Teaching/2019-spring/eng-682/tutorials/data/wright-txt/VAC5884.txt",sep="\n",what="raw")

```

Here, we're creating a new variable called `text` by scanning in the data from the file `VAC5884.txt`. Additional arguments we've included are to separate the text by new line (`\n`) and we're telling that the data type is `raw`, which means, essentially, unformatted text. You can take a look at what was imported by highlighting the word `text` above and hitting `control+enter`. You'll notice each paragraph is a new line, and there are also some blank lines, as indidcated by empty quotation marks.

## Tokenization

Having the full text is nice, but in order to perform some analysis on this text, it will be helpful to break the text up into words. Breaking a piece of text into words is called "tokenizing." There are many ways to do it, but the simplest is to simply to remove anything that isn't a letter. Using regular expression syntax, the R function `strsplit` lets us do just this: split a string into pieces. We'll use the regular expression `[^A-Za-z]` to say "split on anything that isn't a letter between A and Z." Note, for example, that this makes the word "Don't" into two words.

```{r}

test <- text %>% 
  strsplit("[^A-Za-z]")

```

## Aside: Regular Expressions

Regular expressions are a whole aspect of text analysis unto themselves, one that we just don't have the time to cover in this course. In brief, they are a way of abstractly describing the structure of texts, and you can use this abstract description to search for patterns in the text. Regular expression searching, or regex'ing, is a powerful way to clean your data and prepare it for analysis. You must do this with almost any data you find, so understanding how regex'ing works, and being able to use it, is essential for doing computational research in the humanities (it is just not essential for us to learn in this particular class). If you're interested in learning more, I recommend the following resources:

1. Doug Knox's ["Understanding Regular Expressions"](http://programminghistorian.org/lessons/understanding-regular-expressions) tutorial at the Programming Historian provides a nice introduction into the basics of RegEx for cleaning historical data.
2. [Regular Expressions 101](https://regex101.com/) allows you to test expressions and breaks down precisely what they're doing in the Explanation and Match Information panels. It helps to make transparent what RegEx actually does.
3. This [Regular Expressions Quick Start](http://www.regular-expressions.info/quickstart.html) gives a useful overview of some core RegEx stuff, and the larger resource delves into lots more detail.
4. Once you understand the basics of RegEx matching, this [cheat sheet](http://web.mit.edu/hackl/www/lab/turkshop/slides/regex-cheatsheet.pdf) may help you recall precisely the characters you need for particular patterns.
5. Ryan Cordell also wrote a [RegEx tutorial](https://github.com/rccordell/s17hda/blob/master/exercises/RegEx.Rmd) for his Humanities Data Analysis course in 2017. I recommend it!

## Converting from List to Character

Ok, back to the text. After running the code block above (the one with the regular expression), you'll notice that now each paragraph is broken off in a strange way. The above function created a list in which the paragraph (or line) is still the top level and nested below is another list of all the words in that paragraph. If we use the `unlist` function, we convert this from a list to a character string that includes all the words.

```{r}

words <- text %>% 
  strsplit("[^A-Za-z]") %>% 
  unlist 

```

## Converting from Character to Data Frame

Let's take this one step further and coerce the data into a format we're more used to working with, a data frame:

```{r}

wrightWords <- data.frame(word=words,stringsAsFactors = "false")

```

Now we've created a data frame called `wrightWords`. The `stringsAsFactors` argument tells it that we don't want to convert the strings to factors and instead want to leave them as characters. 

## Removing Blank Observations

If you look at that data frame, you'll notice that each word appears in its own row or observation, but there are also a lot blank observations. This is because of all the spaces in the original document. Let's use a filter to get rid of all of those:

```{r}

wrightWords <- wrightWords %>% filter(word != "")

```

Here, we're overwriting the data frame with a new data frame that we've filtered to include only words that are not equal to (`!=`) nothing. This is a good time to explain the syntax you see here for not equal to. In R, as well as in a lot of other programming languages, to indicate that something is equal you use two equal signs (`==`). Not equal, as we've seen, is `!=`. You can also use greater than (`>`), less than (`<`), greater than or equal to (`>=`), or less than or equal to (`<=`).

## Word Counts

Okay, now that we have this in a data frame with all the words and no blank spaces, let's start doing some analysis. One of the most basic forms of analysis is counting words. In order to do this we're going to need to pipe together a few functions. First, we'll create a new data frame called `wordcounts`. Then because we want to count the total number of each word, we'll use `group_by(word)`. Here, `word` is the name of the column, or variable, and just as it says, this arranges variables into groups. Next, we use the `summarize()` function, which summarizes multiple values into a single value, and the single value we want is going to be called `count`. `n()` is a variable that means the number of obserations in a group. Finally, we're using the `arrange()` function to arrange the data frame by the `count` column in descending order (as indicated by the `-`). Try it out:

```{r}

wordcounts <- wrightWords %>% group_by(word) %>% dplyr::summarize(count=n()) %>% dplyr::arrange(-count)

```

## Visualizing Word Counts

Now, let's take a look at the top few rows of that data frame.

```{r}
# inspect the top rows, or should we say the *head*, of the wordcounts data frame 


```

You won't be terribly surprised to find that the most common words are "the", "of", "and", "to, "in", "a", and so on. There are ways to filter out these most common words, which we briefly touched on in tutorial 1, but for now we'll let them stand. Now that we have these word counts, let's try something fun. Using the `ggplot2` package (included in `tidyverse`, so we don't need to load it if we've already loaded up `tidyverse`) we can plot the most common words. I'm not going to go into too much detail about what is going on here since it's not super important for our purposes, but let's run the following commands:

```{r}

wordcounts <- wordcounts %>% mutate(rank = rank(-count)) %>% filter(count>2,word!="")

ggplot(wordcounts) + aes(x=rank,y=count,label=word) + geom_text() + scale_x_continuous(trans="log") + scale_y_continuous(trans="log")

```

You'll notice we've added a column called `rank` using the `mutate()` function and used the `rank()` function to assign a rank based on the count in descending order. When you plot the count over rank you should see an interesting pattern. The logarithm of rank decreases linearily with the logarithm of count. 

This is "Zipf's law:" the phenomenon means that the most common word is twice as common as the second most common word, three times as common as the third most common word, four times as common as the fourth most common word, and so forth. 

It is named after the linguist George Zipf, who first found the phenomenon while laboriously counting occurrences of individual words in Joyce's *Ulysses* in 1935. Not super relevant, but sort of interesting.

# Building Concordances

Our last experiment with the words from this singular text will be to create a concordance. A concordance is just an alphabetical list of the words in a text that references the context in which these words occur. So, for example, a concordance allows you to see every time the word `boring` appears in a text and where exactly it occurs (including the other words that occur around it). We can create a concordance by adding another column to the frame which is not just the first word, but the second. The `dplyr` package includes a `lag` and `lead` function that let you combine the next element. You specify by how many positions you want a vector to "lag" or "lead" another one. Try it below:

```{r}
numbers <- c(1,2,3,4,5)
lag(numbers,1)
lead(numbers,1)
```

What happened in the code block above? Write an exaplanation of each line below.

### A:



By using `lead` on a character vector, we can neatly align one series of text with the words that follow. Below we use `mutate()` again to add another new column to our `wrightWords` data frame, which we call `word2` and indicate that the value of that column should be the value of `word` led by 1.

```{r}

wrightWords %>% mutate(word2 = lead(word,1)) %>% head

```

If we add multiple lead columns we can construct our concordance:

```{r}

multiColumn <- wrightWords %>% mutate(word2 = lead(word,1),word3=lead(word,2),word4=lead(word,3))

```

Inspect the `multiColumn` variable using `View()` to see what this looks like.

You can get context around a certain word as follows:

```{r}

multiColumn %>% filter(word3=="sea")

```

Apparently, the word "sea" appears twice in the text in phrases such as "the wide sea of" and "the unrelenting sea must".

Describe what the above code block is doing in your own words. Then answer the following question: What is the function of the `word3=="sea"` part of the code? What is this doing?

### A:



## Reading in Multiple Texts

Working with a single text is fun, but the real magic happens when you have a set of texts to work with. We begin by creating a list of all the text files we want to read in (all of the files in the `wright-full` folder, within the larger `data` folder). Before you run the code, though, look at the filepath. Where do you need to point your computer? What data are we trying to read in?

```{r}

WRIGHT <- list.files("/Users/lxt308/Dropbox/Teaching/2019-spring/eng-682/tutorials/data/wright-txt/",full.names = TRUE)

```

Using the `list.files()` function, we point to the folder where our files are stored and create a list of all the filenames there. 

## Building a Function

Next, we're going to build our own function. This gets complicated, but what you really need to know is that a function is used to perform several operations at once. It is like a kind of step-by-step guide designed to accomplish particular tasks that you can call on over and over again to do those tasks.

We have used lots of pre-made functions thus far -- like the `mutate` function, for example, or even just the `help()` function -- but R also allows you to write your own. We could even save this function in a separate `.R` file and call it from lots of different scripts. This is one way to store operations you use frequently so you don't have to rewrite the code for them each time you need them. This is only important for us to be aware of so that when we see functions called in other people's scripts, we know what's happening. Let's run the following:

```{r}

readWRIGHTtext <- function(file) {
  message(file)
  text = paste(scan(file, sep="\n",what="raw",strip.white = TRUE))
  WRIGHT = data.frame(filename=file,text=text,stringsAsFactors = FALSE) %>% group_by(filename) %>% summarise(text = paste(text, collapse = " "))
  return(WRIGHT)
}

```

The first thing to note here is that after you run the above code it will look like nothing has happened. But, what *has* happened is that function has been stored by R for later use. You can see it if you scroll to the bottom of your "Environment" window (to the right for most of you). 

This particular function takes as its starting point the individual file. Then it creates a variable called `text` in which it scans the contents of the file (just like we did above). Then, it creates a kind of temporary data frame, in this case called `WRIGHT`, wherein there are two columns "filename", which will be the name of the file and "text." 

Another thing to note about the above function is that it doesn't refer to any specifics. Notice, for example, that the word `file` doesn't refer to a *specific* file, but instead acts as a kind of placeholder for user input (when running the function, the user would replace `file` with the file they want to read in). This makes the function reusable with any text file.

## Running the Function

In order to run the function, we do the following: 

```{r}

allWRIGHTtext <- data.frame(filename=WRIGHT,stringsAsFactors=FALSE) %>% 
  group_by(filename) %>% 
  do(readWRIGHTtext(.$filename)) 

```

Here we are creating a new data frame that uses the temporary data frame created by the function `WRIGHT` as its starting point, then it pulls together (`group_by(filename)`) all the texts by file name and runs the function (`do()`). The output will be a new data frame called `allWRIGHTtext` with two columns, "filename" and "text". 

Now is as good a time as any to talk about code reuse. The above function was originally created by Ben Schmidt (History, Northeastern) for a graduate course he taught. Jonathan Fitzgerald, a student in that course and the person originally wrote much of this tutorial, has adapted and used it over and over again in the years since. Maybe this is obvious to you by now, but when writing code there's never a need to reinvent the wheel. Most likely, someone else has done what you want to do before.

## Modifying the Function to Tokenize

Speaking of not reinventing the wheel, you'll notice that the above function outputs the full text of each file, but as above, we probably want to tokenize this to work with words. We can build this feature right into the function with a few small tweaks:

```{r}

readWRIGHTwords <- function(file) {
  message(file)
  text = paste(scan(file, sep="\n",what="raw",strip.white = TRUE),collapse = "/n")
  words = text %>% 
    strsplit("[^A-Za-z]") %>% 
    unlist 
  WRIGHT = data.frame(word=words,filename=gsub("/Users/lxt308/Dropbox/Teaching/2019-spring/eng-682/tutorials/data/wright-txt/","",file),stringsAsFactors = FALSE) %>% filter(word != "")
  return(WRIGHT)
}

```

We've created a new function called `readWRIGHTwords` (as opposed to `readWRIGHTtext` as above). You'll notice that in the "functions" section of your environment, this has been added. What have we changed (note the `gsub()`, which is a function for searching using regular expressions)? Write down the differences between these two functions below.

### A:


Now let's run that new function:

```{r}

allWRIGHTwords <- data.frame(filename=WRIGHT,stringsAsFactors=FALSE) %>% 
  group_by(filename) %>% 
  do(readWRIGHTwords(.$filename))

```

# Unique Words

Go ahead and `View()` the object (`allWRIGHTwords`) we just created. What is it? Now that we have a data frame with all the words for all the files, it might be kind of interesting to see what words are unique to certain texts.

```{r}

unique <- allWRIGHTwords %>% 
  mutate(word=tolower(word)) %>%
  distinct(word) %>% 
  group_by(word) %>% 
  filter(n()==1) %>% 
  arrange(word)

```

Here we are piping together a few functions: first, because we want to see the unique words regardless of their case (we want "The" and "the" to be the same word) we use `mutate()` to replace the column "word" with another column "word" in which all words are lowercase. That is what the function `tolower()` does for us. Then, we use another handy function `distinct()` to find unique (or distinct) rows. Finally, we group those distinct words together and filter to select just one. This last step is necessary because a word that is distinct to a text might appear more than once in that text, thus giving us an inaccurate count. 

Go ahead and `View()` this object (`unique`). Something you're likely to notice off the bat is there are a lot of what look like nonsense or non-words in there, like `aad` and `addvunt`. These might be typos, or tokenization errors, or they might have to do with something specific from one or more of the texts (i.e., they are meaningful in that context). It's likely, however, that many of these are optical character recognition (OCR) errors. Meaning, if a text was scanned as a pdf and then submitted to an OCR program, sometimes the OCR program read words wrong and created nonsense or non-words. There are ways to filter these words out, but this is something to be aware of in any text analysis project, but especially one using data that has been submitted to OCR.

# Exercises

1. This last exercise is about reading (and trying to understand) someone else's code. In the `data` folder in our course repo (what you downloaded from Github, the same thing that contains these tutorials), you will find the R script we briefly looked at in tutorial 1 (NOT an RMD file) called `sci-lit-2018-class.R`. Open that script up in RStudio. You will also find a file called `Brief_History_socialnetwork_links.csv`. This is the file you will load when you run the script.

This script is something I (Lindsay) wrote to help me out with a project I was working on this past summer. It uses a social network graph of the novel *A Brief History of Seven Killings* by Marlon James (2014) I produced using someone else's software (referred to as lit-cascades in the script), and performs some additional, very basic social network analysis of that graph. 

Perhaps the first thing you will notice when you start reading this script is that I am a pretty terrible R programmer. I don't even do anything fancy like use pipe operators or write my own functions. To that I say: who cares! What I am showing you is not a shining example of excellent code that you should strive to emulate; doubtless there are other, more efficient and elegant ways to accomplish what I do in this script. I have been learning some of them along with you via these tutorials (as a self-taught programmer, I am always learning something new). Rather, it is simply an example of some R code that accomplishes a job, however messily (in this case, some social network analysis). I hope this emphasizes that you do not have to be a great programmer to do this kind of work, or even a very good one, as long as you understand the concepts behind the methods.

Your goal for this exercise is to try to follow along with what I've done and explain its logic. We're only going to do this for part of the script, lines 1-71. In these lines, you will encounter some new functions that we haven't discussed. You may not even know what social network analysis is (if that's the case, and if you want to know, I recommend [this piece](http://www.scottbot.net/HIAL/index.html@p=6279.html) by Scott Weingart to give you some vocabulary). All of that is fine. You don't need to fully understand social network analysis to follow along with the data transformations this script makes possible. For each function I've listed below, write, in your own words, a brief explanation of that function and how I use it in the script. That's it. 

You're welcome to explore the script further, and I've given you all of the social network data that I used in your `data` folder (I analyzed six different novels in all), so feel free to use it if you want. But for this tutorial, just work with the `Brief_History` data and only focus on lines 1-71.

### The Functions:

For each function I've listed below, write, in your own words, a brief explanation of that function, what package it comes from (if it comes from a specific package; some of the functions below are base R functions), and how I use it in the script.

`read.csv()`:

`graph_from_data_frame`:

`summary`:

`plot`:

`strength`:

`gsize`:

`write.table`:

`barchart`:

Send the completed, rendered HTML version of your tutorial to me by class on Monday, March 4.

## Posible Rendering Errors

When you try to render this document to send it to me as an HTML file, you may encounter these errors (visible in your R Markdown pane):

* `Error in setwd("") : cannot change working directory`: If there isn't anything in the quotes in `setwd("")`, then that is your problem. The program can't set your working directory because there's nothing there.
* If you are seeing an error about XQuartz: This can happen if you have a Mac. The easiest fix is to comment out any `View()` functions in any code blocks and try rendering again.
* If you see an error about not being able to install packages: Comment out any lines in code blocks where you install packages and try again.
